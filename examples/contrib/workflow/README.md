# Karl Workflow Engine üöÄ

> **Production-grade workflow orchestration with advanced retry policies, worker pools, and state persistence**

A comprehensive workflow engine demonstrating Karl's powerful concurrency features, built for real-world production workloads.

---

## üìã Table of Contents

- [Quick Start](#quick-start)
- [Overview](#overview)
- [Core Features](#core-features)
- [Workflow Types](#workflow-types)
- [Advanced Features](#advanced-features)
- [Configuration](#configuration)
- [Examples](#examples)
- [Architecture](#architecture)
- [Testing](#testing)
- [Use Cases](#use-cases)

---

## üöÄ Quick Start

```bash
# Run the quick start guide
cd examples/contrib/workflow/

karl run quickstart.k

# Try specific examples
karl run examples.k           # Basic workflows
karl run test_pipeline.k      # Pipeline execution
karl run test_integrated_features.k  # All features

# Explore advanced demos
karl run timer_tasks.k        # Scheduled tasks
karl run dag_pipeline.k       # Complex DAG
karl run subdag_demo.k        # Reusable components
```

---

## üìñ Overview

The Workflow Engine provides **six execution modes** for orchestrating tasks:

| Mode | Description | Use Case |
|------|-------------|----------|
| **Sequential** | Tasks run one after another | ETL pipelines, step-by-step processing |
| **Parallel** | Tasks run concurrently | API calls, independent operations |
| **DAG** | Dependency-based execution | Complex workflows with prerequisites |
| **Pipeline** | Multi-stage with worker pools | High-throughput data processing |
| **Timer** | Delayed or scheduled execution | Notifications, deferred processing |
| **Interval** | Repeated execution | Health checks, monitoring, polling |

---

## ‚ú® Core Features

### Production-Grade Capabilities

- ‚úÖ **Dependency Resolution** - Tasks wait for prerequisites to complete
- ‚úÖ **Parallel Execution** - Worker pools for concurrent processing  
- ‚úÖ **Advanced Retry Policies** - Exponential back-off, circuit breakers, jitter
- ‚úÖ **State Persistence** - Save/resume workflows, checkpoint recovery
- ‚úÖ **Error Handling** - Configurable retries and graceful degradation
- ‚úÖ **Context Passing** - Results flow between tasks
- ‚úÖ **Sub-DAGs** - Reusable workflow components
- ‚úÖ **Performance Metrics** - Built-in monitoring and profiling

### üéØ Enhanced Features (2026-02-04)

#### 1. **Persisted DAG State**
- JSON-based workflow state storage
- Automatic checkpointing every 5 completed nodes
- Resume interrupted workflows from last checkpoint
- Audit trails and workflow debugging

#### 2. **Advanced Retry Policies**
- Three retry strategies: Fixed, Linear, Exponential
- Jitter support to prevent thundering herd
- Circuit breaker pattern for automatic failure recovery
- Configurable min/max delay caps

#### 3. **Parallel Execution Engine**
- Worker pool management with configurable workers
- Efficient task queue and load balancing
- Batched execution for high-throughput processing
- Per-worker performance metrics

### üîß Integration Philosophy

All features are **fully integrated** into the main engine while maintaining:
- **Backward Compatibility** - Legacy workflows run unchanged
- **Opt-in Features** - Each feature can be enabled independently
- **Modular Design** - Features can be used standalone or combined
- **Zero Breaking Changes** - Existing code continues to work

---

## üì¶ Workflow Types

### 1. Sequential Workflows

Tasks execute one after another, passing results forward.

```karl
let workflow = {
    name: "ETL Pipeline",
    type: "sequential",
    tasks: [
        {
            name: "Extract",
            handler: (ctx) -> {
                success: true,
                data: { records: 1000 }
            },
        },
        {
            name: "Transform",
            handler: (ctx) -> {
                // ctx contains result from Extract
                success: true,
                data: { processed: ctx.data.records * 2 }
            },
        },
        {
            name: "Load",
            handler: (ctx) -> {
                success: true,
                data: ctx.data
            },
        },
    ],
}

engine.execute(workflow, {}, {})
```

**When to use:** Simple step-by-step processes where each step needs the previous result.

---

### 2. Parallel Workflows

All tasks run concurrently and results are collected.

```karl
let workflow = {
    name: "Multi-API Fetch",
    type: "parallel",
    tasks: [
        {
            name: "Fetch Users",
            handler: (ctx) -> fetchAPI("/users"),
        },
        {
            name: "Fetch Products",
            handler: (ctx) -> fetchAPI("/products"),
        },
        {
            name: "Fetch Orders",
            handler: (ctx) -> fetchAPI("/orders"),
        },
    ],
}
```

**When to use:** Independent operations that can run simultaneously.

---

### 3. DAG Workflows (Directed Acyclic Graph)

Tasks execute based on dependencies, with parallel execution where possible.

```karl
let workflow = {
    name: "Data Processing DAG",
    type: "dag",
    nodes: [
        { id: "fetch-users", name: "Fetch Users", handler: (ctx) -> {...} },
        { id: "fetch-orders", name: "Fetch Orders", handler: (ctx) -> {...} },
        { id: "merge", name: "Merge Data", handler: (ctx) -> {...} },
        { id: "analyze", name: "Analyze", handler: (ctx) -> {...} },
    ],
    edges: [
        { source: "fetch-users", target: "merge" },
        { source: "fetch-orders", target: "merge" },
        { source: "merge", target: "analyze" },
    ],
}
// fetch-users and fetch-orders run in parallel
// merge waits for both to complete
// analyze waits for merge
```

**When to use:** Complex workflows where some tasks depend on others.

---

### 4. Pipeline Workflows

Multi-stage data processing with worker pools per stage.

```karl
let workflow = {
    name: "ETL Pipeline",
    type: "pipeline",
    stages: [
        {
            name: "Extract",
            workers: 2,
            handler: (item) -> {
                success: true,
                data: { id: item.id, extracted: true }
            }
        },
        {
            name: "Transform",
            workers: 3,
            handler: (item) -> {
                success: true,
                data: { id: item.id, transformed: true }
            }
        },
        {
            name: "Load",
            workers: 2,
            handler: (item) -> {
                success: true,
                data: { id: item.id, loaded: true }
            }
        },
    ],
    items: [{ id: 1 }, { id: 2 }, { id: 3 }],
}
```

**When to use:** High-throughput data processing with multiple stages.

---

### 5. Timer Tasks

Execute tasks after a delay or at intervals.

```karl
// Delayed execution
let delayedWorkflow = {
    name: "Delayed Notification",
    type: "timer",
    task: {
        name: "Send Email",
        delay: 5000,  // Wait 5 seconds
        handler: (ctx) -> {
            sendEmail()
            { success: true, data: "Email sent" }
        },
    },
}

// Interval execution (repeated)
let intervalWorkflow = {
    name: "Health Monitor",
    type: "interval",
    task: {
        name: "Check Health",
        interval: 1000,        // Every 1 second
        maxRepetitions: 10,    // Run 10 times
        handler: (ctx) -> {
            log("Health check #", ctx.iteration)
            checkSystemHealth()
            { success: true, data: { iteration: ctx.iteration } }
        },
    },
}
```

**When to use:**
- **Timer:** Scheduled notifications, deferred processing, rate limiting
- **Interval:** Health checks, monitoring, polling, periodic updates

---

### 6. Sub-DAGs (Reusable Components)

Create workflow components once and reuse them anywhere.

```karl
// Define reusable validation component
let validationSubDAG = createSubDAG(
    "validation",
    "Data Validation",
    [
        { id: "check-format", name: "Format", handler: validateFormat },
        { id: "check-schema", name: "Schema", handler: validateSchema },
        { id: "check-integrity", name: "Integrity", handler: checkIntegrity },
    ],
    [
        { source: "check-format", target: "check-schema" },
        { source: "check-schema", target: "check-integrity" },
    ]
)

// Use in a larger workflow
let workflow = {
    name: "ETL with Validation",
    type: "dag-with-subdags",
    nodes: [
        { id: "extract", name: "Extract", handler: extractData },
        validationSubDAG,  // <- Reusable component!
        { id: "load", name: "Load", handler: loadData },
    ],
    edges: [
        { source: "extract", target: "validation" },
        { source: "validation", target: "load" },
    ],
}
```

**When to use:** Modular workflows, shared validation/transformation logic, testing.

---

## üéØ Advanced Features

### 1. Advanced Retry Policies

Intelligent retry strategies with exponential back-off, jitter, and circuit breakers.

#### Exponential Back-off

```karl
import "examples/contrib/workflow/retry_policy.k" as Retry

let config = {
    retryPolicy: {
        maxAttempts: 5,
        strategy: Retry.RETRY_EXPONENTIAL,  // or RETRY_LINEAR, RETRY_FIXED
        initialDelay: 1000,      // 1 second
        maxDelay: 30000,         // 30 seconds max
        jitterEnabled: true,     // Add randomness to prevent thundering herd
        jitterFactor: 0.1,       // 10% jitter
        retryableErrors: [],     // Retry all errors (or specify specific ones)
    }
}

let workflow = {
    name: "Resilient API Workflow",
    type: "sequential",
    tasks: [
        {
            name: "Call External API",
            handler: (ctx) -> {
                // This will retry with exponential back-off on failure
                callExternalAPI()
            }
        }
    ]
}

engine.execute(workflow, {}, config)
```

**Retry Strategies:**
- **Fixed**: Same delay between retries (e.g., 1s, 1s, 1s)
- **Linear**: Delay increases linearly (e.g., 1s, 2s, 3s)
- **Exponential**: Delay doubles each time (e.g., 1s, 2s, 4s, 8s)

**Jitter**: Adds randomness to prevent multiple clients from retrying simultaneously.

#### Circuit Breaker Pattern

```karl
import "examples/contrib/workflow/retry_policy.k" as Retry

let circuitBreaker = Retry.createCircuitBreaker({
    threshold: 5,           // Open circuit after 5 failures
    timeout: 60000,         // Wait 60s before trying again
    halfOpenAttempts: 3,    // Test with 3 attempts in half-open state
})

// Execute task through circuit breaker
let result = circuitBreaker.execute(task, context)

// Circuit states: CLOSED (normal) -> OPEN (failing) -> HALF_OPEN (testing)
```

**Use Cases:**
- Resilient API calls
- Network failure handling
- Transient error recovery
- Distributed system coordination

---

### 2. Parallel Execution with Worker Pools

Efficient multi-core task execution with configurable worker pools and task queues.

#### Worker Pool Configuration

```karl
let config = {
    useWorkerPool: true,      // Enable worker pool mode
    workerCount: 4,           // Number of concurrent workers
    queueSize: 100,           // Task queue size
    enableMetrics: true,      // Collect performance metrics
}

let workflow = {
    name: "Parallel Processing",
    type: "parallel",
    tasks: [
        { name: "Task 1", handler: (ctx) -> processData(1) },
        { name: "Task 2", handler: (ctx) -> processData(2) },
        { name: "Task 3", handler: (ctx) -> processData(3) },
        { name: "Task 4", handler: (ctx) -> processData(4) },
        { name: "Task 5", handler: (ctx) -> processData(5) },
        { name: "Task 6", handler: (ctx) -> processData(6) },
        { name: "Task 7", handler: (ctx) -> processData(7) },
        { name: "Task 8", handler: (ctx) -> processData(8) },
    ]
}

let result = engine.execute(workflow, {}, config)

// Access worker metrics
if result.metrics {
    for i < result.metrics.length with i = 0 {
        let workerMetrics = result.metrics[i]
        log("Worker", workerMetrics.workerId, "processed", workerMetrics.metrics.tasksProcessed, "tasks")
        i = i + 1
    } then {}
}
```

**Benefits:**
- **Resource Control**: Limit concurrent tasks to prevent overwhelming the system
- **Load Balancing**: Tasks are distributed evenly across workers
- **Metrics**: Track per-worker performance and throughput
- **Efficiency**: Reuse workers instead of spawning goroutines per task

#### Batched Execution

```karl
import "examples/contrib/workflow/parallel_executor.k" as Parallel

let executor = Parallel.createParallelExecutor({
    workerCount: 4,
    batchSize: 10,
})

// Process 100 tasks in batches of 10
let result = executor.executeBatched(tasks, context)
```

**Use Cases:**
- Multi-core CPU utilization
- High-throughput data processing
- Resource-constrained environments
- Performance optimization

---

### 3. Persisted DAG State

Save and resume workflows with automatic checkpointing.

#### Basic Persistence

```karl
let config = {
    enablePersistence: true,
    workflowId: "my-etl-pipeline-001",
    storageConfig: {
        storageDir: "./workflow-state",
        enableAutoCheckpoint: false,
        checkpointInterval: 5000,
        compressionEnabled: false,
    }
}

let workflow = {
    name: "Long Running ETL",
    type: "dag",
    nodes: [...],
    edges: [...],
}

let result = engine.execute(workflow, {}, config)

// State is automatically saved to: ./workflow-state/my-etl-pipeline-001.json
log("Workflow ID:", result.workflowId)
```

#### Resume from Checkpoint

```karl
import "examples/contrib/workflow/storage.k" as Storage

let storage = Storage.createStorageEngine({
    storageDir: "./workflow-state",
})

// Check if workflow can be resumed
let resumeCheck = storage.canResume("my-etl-pipeline-001")

if resumeCheck.resumable {
    log("Found saved state!")
    log("Completed:", resumeCheck.state.totalCompleted, "nodes")
    
    // Get nodes that still need to run
    let incompleteNodes = storage.getIncompleteNodes(nodes, resumeCheck.state)
    log("Remaining:", incompleteNodes.length, "nodes")
    
    // Execute with same workflowId to resume
    let result = engine.execute(workflow, {}, config)
} else {
    log("Starting fresh workflow")
}
```

#### Automatic Checkpointing

The DAG executor automatically creates checkpoints every 5 completed nodes:

```karl
// Checkpoint is created automatically during execution
// State includes:
// - completedNodes: which nodes have finished
// - startedNodes: which nodes are in progress
// - results: output from completed nodes
// - totalCompleted: count of finished nodes
```

**Use Cases:**
- Long-running ETL pipelines
- Crash recovery
- Workflow debugging and inspection
- Audit trails

---

### Combined Features Example

Use all three features together for maximum resilience:

```karl
import "examples/contrib/workflow/retry_policy.k" as Retry

let config = {
    // Retry policy
    retryPolicy: {
        maxAttempts: 3,
        strategy: Retry.RETRY_EXPONENTIAL,
        initialDelay: 1000,
        maxDelay: 10000,
        jitterEnabled: true,
    },
    
    // Worker pool (for parallel tasks)
    useWorkerPool: true,
    workerCount: 4,
    queueSize: 100,
    enableMetrics: true,
    
    // State persistence
    enablePersistence: true,
    workflowId: "resilient-etl-pipeline",
    storageConfig: {
        storageDir: "./workflow-state",
        checkpointInterval: 5000,
        compressionEnabled: false,
    },
}

let workflow = {
    name: "Resilient ETL Pipeline",
    type: "dag",
    nodes: [
        { id: "fetch-1", name: "Fetch Source 1", handler: fetchData1 },
        { id: "fetch-2", name: "Fetch Source 2", handler: fetchData2 },
        { id: "validate", name: "Validate", handler: validateData },
        { id: "transform", name: "Transform", handler: transformData },
        { id: "load", name: "Load", handler: loadData },
    ],
    edges: [
        { source: "fetch-1", target: "validate" },
        { source: "fetch-2", target: "validate" },
        { source: "validate", target: "transform" },
        { source: "transform", target: "load" },
    ],
}

let result = engine.execute(workflow, {}, config)

// This workflow will:
// - Retry failed tasks with exponential back-off
// - Execute fetch-1 and fetch-2 in parallel
// - Save state periodically (every 5 nodes)
// - Can be resumed if interrupted
// - Collect performance metrics
```

---

## ‚öôÔ∏è Configuration

### Engine Configuration

```karl
let config = {
    // Legacy retry settings
    defaultRetries: 2,        // Simple retry count
    defaultWorkers: 3,        // Worker pool size for pipelines
    stopOnError: false,       // Continue on failures
    
    // Enhanced features
    retryPolicy: null,           // Advanced retry (see Retry module)
    useWorkerPool: false,        // Use worker pool for parallel execution
    workerCount: 4,              // Number of workers in pool
    queueSize: 100,              // Task queue size
    batchSize: 10,               // Batch size for processing
    enablePriority: false,       // Enable priority queue
    shutdownTimeout: 30000,      // Shutdown timeout in ms
    enableMetrics: true,         // Collect performance metrics
    enablePersistence: false,    // Enable state persistence
    workflowId: null,            // Unique workflow identifier
    storageConfig: {             // Storage configuration
        storageDir: "./workflow-state",
        enableAutoCheckpoint: false,
        checkpointInterval: 5000,
        compressionEnabled: false,
    },
}

engine.execute(workflow, initialContext, config)
```

### Task Configuration

```karl
let task = {
    name: "Task Name",
    retries: 3,               // Override default
    handler: (ctx) -> {
        // Task logic
        { success: true, data: result }
    },
}
```

---

## üìö Examples

### Basic Examples (`examples.k`)
Demonstrates core workflow patterns:
- ‚úÖ Sequential ETL pipeline
- ‚úÖ Parallel API requests
- ‚úÖ Mathematical pipeline with validation
- ‚úÖ Retry mechanism
- ‚úÖ Parallel data aggregation

```bash
karl run examples/contrib/workflow/examples.k
```

---

### Timer Tasks (`timer_tasks.k`)
Comprehensive timer demonstrations:
- Delayed task execution
- Interval-based health monitoring  
- Coordinated batch processing
- Multi-timer coordination
- Periodic status updates

```bash
karl run examples/contrib/workflow/timer_tasks.k
```

---

### DAG Pipeline (`dag_pipeline.k`)
Advanced multi-stage data processing:
- **Stage 1:** Parallel data fetching (4 sources)
- **Stage 2:** Fan-out to 2 processing paths
- **Stage 3:** Worker pool transformation (4 workers)
- **Stage 4:** Parallel aggregation (4 metrics)
- **Stage 5:** Report generation

```bash
karl run examples/contrib/workflow/dag_pipeline.k
```

---

### Sub-DAG Demo (`subdag_demo.k`)
Reusable workflow components:
- Single sub-DAG execution
- Sequential sub-DAG pipelines
- Parallel sub-DAG execution
- Nested sub-DAGs (3 levels deep)
- ETL and ML pipeline patterns

```bash
karl run examples/contrib/workflow/subdag_demo.k
```

---

### Quick Start (`quickstart.k`)
Perfect for beginners:
1. Simple delayed task
2. Interval-based execution  
3. Basic sub-DAG usage
4. Combined timer + sub-DAG

```bash
karl run examples/contrib/workflow/quickstart.k
```

---

## üèóÔ∏è Architecture

### Karl Language Features

#### Concurrency
```karl
& taskFunction()          // Spawn async task
wait task                 // Wait for completion
rendezvous()              // Create unbuffered channel
buffered(size)            // Create buffered channel
ch.send(value)            // Send to channel
ch.recv()                 // Receive from channel
ch.done()                 // Close channel
```

#### Control Flow
```karl
for i < 10 with i = 0 {
    // Loop body
    i = i + 1
} then result                // Loop returns value

break value                  // Early exit with result

match workflow.type {
    case "sequential" -> ...,
    case "parallel" -> ...,
    case _ -> ...
}
```

#### Map Methods
```karl
let m = map()
m = m.set("key", value)  // Set property
let val = m.get("key")   // Get property
let has = m.has("key")   // Check existence
```

### Architecture Patterns

#### Worker Pool
```karl
let workChan = buffered(100)

// Spawn workers
for i < numWorkers with i = 0 {
    & (() -> {
        for processing = true with processing = true {
            let [item, done] = workChan.recv()
            if done { break {} }
            processItem(item)
        }
    })()
    i = i + 1
} then {}

// Distribute work
for i < items.length with i = 0 {
    workChan.send(items[i])
    i = i + 1
} then {}

workChan.done()
```

#### Fan-out/Fan-in
```karl
// Fan-out: spawn parallel tasks
let tasks = for i < items.length with i = 0, workers = [] {
    workers += [& processItem(items[i])]
    i = i + 1
} then workers

// Fan-in: collect results
let results = for i < tasks.length with i = 0, collected = [] {
    collected += [wait tasks[i]]
    i = i + 1
} then collected
```

---

## üß™ Testing

### Test Suite

All workflow tests are passing:

```bash
# Run individual tests
karl run examples/contrib/workflow/test_sequential.k
karl run examples/contrib/workflow/test_retry.k
karl run examples/contrib/workflow/test_dag.k
karl run examples/contrib/workflow/test_retry_module.k
karl run examples/contrib/workflow/test_pipeline.k
karl run examples/contrib/workflow/test_integrated_features.k
```

### Test Coverage

- **test_sequential.k** - Basic sequential execution
- **test_retry.k** - Retry with exponential backoff
- **test_dag.k** - Basic DAG execution
- **test_retry_module.k** - Retry module standalone
- **test_pipeline.k** - Multi-stage pipeline with worker pools
- **test_integrated_features.k** - Comprehensive integration test
  - Test 1: Retry policy with exponential back-off
  - Test 2: Worker pool with 4 workers processing 8 tasks
  - Test 3: DAG persistence with save/load
  - Test 4: Combined features (retry + persistence + parallel)

---

## üíº Use Cases

### ETL Pipelines
Sequential data extraction, transformation, and loading with validation steps.

### API Orchestration
Parallel requests to multiple endpoints with result aggregation.

### Data Processing
Multi-stage pipelines with worker pools for high-throughput processing.

### Scheduled Tasks
Delayed notifications, periodic health checks, batch processing.

### Modular Workflows
Reusable components for validation, transformation, and quality assurance.

### Resilient Systems
Automatic retry with exponential back-off for transient failures.

### Long-Running Jobs
State persistence for crash recovery and workflow resumption.

---

## üìä File Overview

| File | Lines | Purpose | Status |
|------|-------|---------|--------|
| `engine.k` | ~710 | Core workflow engine with integrated features | ‚úÖ |
| `retry_policy.k` | ~350 | Advanced retry strategies & circuit breakers | ‚úÖ |
| `parallel_executor.k` | ~415 | Worker pool-based parallel execution | ‚úÖ |
| `storage.k` | ~350 | State persistence & checkpoint management | ‚úÖ |
| `examples.k` | ~330 | Basic workflow demos | ‚úÖ |
| `timer_tasks.k` | ~300 | Timer demonstrations | ‚úÖ |
| `dag_pipeline.k` | ~430 | Advanced DAG demo | ‚úÖ |
| `csv_pipeline.k` | ~300 | CSV processing | ‚úÖ |
| `file_watcher.k` | ~300 | File monitoring | ‚úÖ |
| `subdag_demo.k` | ~550 | Sub-DAG patterns | ‚úÖ |
| `quickstart.k` | ~150 | Beginner guide | ‚úÖ |
| `test_sequential.k` | ~50 | Sequential test | ‚úÖ |
| `test_retry.k` | ~50 | Retry test | ‚úÖ |
| `test_dag.k` | ~50 | DAG test | ‚úÖ |
| `test_retry_module.k` | ~50 | Retry module test | ‚úÖ |
| `test_pipeline.k` | ~300 | Pipeline test | ‚úÖ |
| `test_integrated_features.k` | ~375 | Integration tests | ‚úÖ |

**Total:** ~5,100 lines of production-grade workflow orchestration code

---

## üìà Statistics

### Code Additions (feat/workflow-persistence-retry-parallel)
- **New Modules**: 3 files, ~1,050 lines
- **Engine Updates**: ~150 lines added
- **Tests**: 6 files, ~900 lines
- **Documentation**: This comprehensive guide
- **Total**: ~2,500 lines added

### Files Modified/Created
1. `storage.k` (new) - 350 lines
2. `retry_policy.k` (new) - 350 lines
3. `parallel_executor.k` (new) - 415 lines
4. `engine.k` (modified) - +150 lines
5. `test_pipeline.k` (new) - 300 lines
6. `test_integrated_features.k` (new) - 375 lines
7. `README.md` (rewritten) - comprehensive guide

---

## üîÑ Recent Updates

### 2026-02-04: Deadlock Fix & Pipeline Tests ‚ú®
- **Fixed deadlock** in parallel executor using buffered channels
- **Added pipeline test** covering multi-stage processing
- **All tests passing** - 6 test files, 100% success rate
- **Rebased on main** - ready to merge

### 2026-02-03: Enhanced Features Release ‚ú®
- **Advanced Retry Policies** with exponential back-off, jitter, and circuit breakers
- **Worker Pool Execution** for efficient multi-core parallel processing
- **State Persistence** with automatic checkpointing and resume capability
- **Integrated into main engine** - all features work seamlessly together
- **New modules:** `retry_policy.k`, `parallel_executor.k`, `storage.k`
- **Comprehensive tests** and documentation

### 2026-01-30: DAG Executor Fix ‚ú®
- **Fixed deadlock** in DAG executor
- **Added Map methods** to Karl language (`.get()`, `.set()`, `.has()`)
- **Simplified DAG logic** with counting-based approach
- **All DAG tests passing** ‚úÖ

### 2026-01-30: Timer & Sub-DAG Features ‚ú®
- **Timer tasks** with delayed and interval execution
- **Sub-DAGs** for reusable workflow components
- **New examples** demonstrating advanced patterns

---

## üöÄ Future Enhancements

### Completed ‚úÖ
- ‚úÖ Advanced retry policies with exponential back-off
- ‚úÖ Worker pools for parallel execution
- ‚úÖ State persistence (save/resume workflows)
- ‚úÖ Built-in metrics collection
- ‚úÖ Circuit breakers for failure recovery
- ‚úÖ Buffered channels for deadlock prevention

### Potential Additions
- Task-level execution timeouts
- Priority queues for task scheduling
- Dynamic routing based on results
- Distributed execution across multiple nodes
- Workflow versioning
- Real-time monitoring dashboard
- Workflow visualization tools

---

## üí° Key Achievements

‚úÖ **Reliability** - State persistence enables crash recovery  
‚úÖ **Resilience** - Exponential back-off handles transient failures  
‚úÖ **Performance** - Worker pools maximize multi-core utilization  
‚úÖ **Production-Ready** - All features integrated and tested  
‚úÖ **Well-Documented** - Comprehensive guide with examples  
‚úÖ **Backward Compatible** - No breaking changes  
‚úÖ **Thoroughly Tested** - 6 test files, all passing  
‚úÖ **Deadlock-Free** - Buffered channels prevent concurrency issues  

---

## üìù Error Handling

The engine provides multiple strategies:

1. **Task-level retries** - Automatic retry with configurable attempts
2. **Graceful degradation** - Continue workflow on non-critical failures
3. **Error propagation** - Failed tasks return error information
4. **Stop-on-error** - Optional immediate halt on first failure
5. **Circuit breakers** - Automatic failure detection and recovery

---

**Author:** Nico  
**Language:** Karl  
**Branch:** `feat/workflow-persistence-retry-parallel`  
**Created:** 2026-01-29  
**Updated:** 2026-02-04  
**Files:** 17  
**Status:** Production Ready ‚úÖ  
**Tests:** All Passing ‚úÖ
